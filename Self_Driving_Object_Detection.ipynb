{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self Driving Object Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9pFeTdza9MzyJat9jc4Wf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andres8bit/Machine-Learning/blob/main/Self_Driving_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBNkBOGI1JvV"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQCfBEdQL2S9"
      },
      "source": [
        "url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n",
        "filename = os.path.join(os.getcwd(), \"data.zip\")\n",
        "keras.utils.get_file(filename, url)\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(\"data.zip\", \"r\") as z_fp:\n",
        "    z_fp.extractall(\"./\")\n"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBeeoBVLQvaR"
      },
      "source": [
        "# Helper Functions for bounding box:\n",
        "def swap_x_y(boxes):\n",
        "  # swaps all the x y values of the boxes\n",
        "  return tf.stack([ boxes[:,1] ,boxes[:,0], boxes[:,3],boxes[:,2]],axis=-1)\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "  # Convert [xmin,ymin,xmax,ymax] -> [x,y,width,height]\n",
        "  return tf.concat(\n",
        "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
        "        axis=-1,\n",
        "    )\n",
        "\n",
        "def convert_to_corners(boxes):\n",
        "  # Convert [x,y,width,height] -> [xmin,ymin,xmax,ymax]\n",
        "    return tf.concat(\n",
        "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
        "        axis=-1,\n",
        "    )  \n"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAKMP0xUT4tp"
      },
      "source": [
        "def intersection_over_union(boxes1,boxes2):\n",
        "  # Calculate the intersection over union between the ground truth and predicted\n",
        "  # anchor boxes\n",
        "  boxes1_corners = convert_to_corners(boxes1)\n",
        "  boxes2_corners = convert_to_corners(boxes2)\n",
        "  lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
        "  rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
        "  intersection = tf.maximum(0.0, rd - lu)\n",
        "  intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
        "  boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
        "  boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
        "  union_area = tf.maximum(\n",
        "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
        "    )\n",
        "  # IOU scores between two box sets of shape (N,M)\n",
        "  return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtwAha51VvtR"
      },
      "source": [
        "def visualize_detections(\n",
        "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
        "):\n",
        "    \"\"\"Visualize Detections\"\"\"\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "    for box, _cls, score in zip(boxes, classes, scores):\n",
        "        text = \"{}: {:.2f}\".format(_cls, score)\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        patch = plt.Rectangle(\n",
        "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
        "        )\n",
        "        ax.add_patch(patch)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1,\n",
        "            text,\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "    plt.show()\n",
        "    return ax"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aucpCYEmWR68"
      },
      "source": [
        "# Anchor Generator:\n",
        "  # Fixed sized boxes that the model uses to predict object bounding boxes\n",
        "  # Regression is used on the offset between the objects center and \n",
        "  # the anchor box.\n",
        "  # Takes the width and height of the anchor box to predict scale of the object.\n",
        "  # Each location within a feature map 9 anchor boxes, in 3 different scales & \n",
        "  # ratios.\n",
        "class AnchorBox:\n",
        "  # Generates Anchor Boxes for features maps of strides [8:128] -> at powers of 2\n",
        "  # Arguments:\n",
        "  #   Aspect ratio: float list for aspect ratios at each feature map location\n",
        "  #   scales: float list for scales of anchor boxes at each location in the feature\n",
        "  #     map.\n",
        "  #   num_anchors: number of anchors at each location of a feature map\n",
        "  #   strides: list of strides of each convolutional layer\n",
        "\n",
        "    def __init__(self):\n",
        "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
        "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
        "\n",
        "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
        "        self._strides = [2 ** i for i in range(3, 8)]\n",
        "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
        "        self._anchor_dims = self._compute_dims()\n",
        "\n",
        "    def _compute_dims(self):\n",
        "        #Computes anchor box dimensions for all ratios and scales at all levels\n",
        "        # of the feature pyramid.\n",
        "        \n",
        "        anchor_dims_all = []\n",
        "        for area in self._areas:\n",
        "            anchor_dims = []\n",
        "            for ratio in self.aspect_ratios:\n",
        "                anchor_height = tf.math.sqrt(area / ratio)\n",
        "                anchor_width = area / anchor_height\n",
        "                dims = tf.reshape(\n",
        "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
        "                )\n",
        "                for scale in self.scales:\n",
        "                    anchor_dims.append(scale * dims)\n",
        "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
        "        return anchor_dims_all\n",
        "\n",
        "    def _get_anchors(self, feature_height, feature_width, level):\n",
        "        # Generates anchor boxes for a given feature map size and level\n",
        "        # Arguments:\n",
        "        #   feature_height: An integer representing the height of the feature map.\n",
        "        #   feature_width: An integer representing the width of the feature map.\n",
        "        #   level: An integer representing the level of the feature map in the\n",
        "        #     feature pyramid.\n",
        "\n",
        "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
        "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
        "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
        "        centers = tf.expand_dims(centers, axis=-2)\n",
        "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
        "        dims = tf.tile(\n",
        "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
        "        )\n",
        "        anchors = tf.concat([centers, dims], axis=-1)\n",
        "        return tf.reshape(\n",
        "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
        "        )\n",
        "\n",
        "    def get_anchors(self, image_height, image_width):\n",
        "        # Generates anchor boxes for all the feature maps of the feature pyramid.\n",
        "\n",
        "        # Arguments:\n",
        "        #   image_height: Height of the input image.\n",
        "        #   image_width: Width of the input image.\n",
        "\n",
        "\n",
        "        anchors = [\n",
        "            self._get_anchors(\n",
        "                tf.math.ceil(image_height / 2 ** i),\n",
        "                tf.math.ceil(image_width / 2 ** i),\n",
        "                i,\n",
        "            )\n",
        "            for i in range(3, 8)\n",
        "        ]\n",
        "        return tf.concat(anchors, axis=0)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc7W5YI_eF7S"
      },
      "source": [
        "# Data Preprocessing:\n",
        "\n",
        "# Horizontal Flip:\n",
        "def random_flip_horizontal(image,boxes):\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    image = tf.image.flip_left_right(image)\n",
        "    boxes = tf.stack(\n",
        "          [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
        "        )\n",
        "  return image, boxes\n",
        "\n",
        "# Resize and Pad Image:\n",
        "def resize_and_pad_image(image, min_side = 800.0, max_side = 1333.0, \n",
        "                         jitter = [640,1024], stride = 128.0):\n",
        "  # image:     3D tensor [height, width, channels].\n",
        "  # min_side:  Size to reshape shorter side of image iff jitter == None.\n",
        "  # max_side:  Size to reshape longer side of image is size is larger than default.\n",
        "  # jitter:    [min,max] size for scale jittering.\n",
        "  # stride:     Size of the smallest stride in the  in the feature pyramid.\n",
        "\n",
        "  image_shape = tf.cast(tf.shape(image)[:2],dtype=tf.float32)\n",
        "\n",
        "  if jitter is not None:\n",
        "    min_side = tf.random.uniform((),jitter[0], jitter[1], dtype=tf.float32)\n",
        "  \n",
        "  ratio = min_side / tf.reduce_min(image_shape)\n",
        "\n",
        "  if ratio * tf.reduce_max(image_shape) > max_side:\n",
        "    ratio = max_side / tf.reduce_max(image_shape)\n",
        "  \n",
        "  image_shape = ratio * image_shape\n",
        "  image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
        "  padded_image_shape = tf.cast(tf.math.ceil(image_shape/stride)*stride,dtype=tf.int32)\n",
        "\n",
        "  image = tf.image.pad_to_bounding_box(image, 0, 0,padded_image_shape[0],padded_image_shape[1])\n",
        "\n",
        "  return image, image_shape, ratio\n",
        "\n",
        "# Process Data:\n",
        "def preprocess_data(sample):\n",
        "    image = sample[\"image\"]\n",
        "    bbox = swap_x_y(sample[\"objects\"][\"bbox\"])\n",
        "    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n",
        "\n",
        "    image, bbox = random_flip_horizontal(image, bbox)\n",
        "    image, image_shape, _ = resize_and_pad_image(image)\n",
        "\n",
        "    bbox = tf.stack(\n",
        "        [\n",
        "            bbox[:, 0] * image_shape[1],\n",
        "            bbox[:, 1] * image_shape[0],\n",
        "            bbox[:, 2] * image_shape[1],\n",
        "            bbox[:, 3] * image_shape[0],\n",
        "        ],\n",
        "        axis=-1,\n",
        "    )\n",
        "    bbox = convert_to_xywh(bbox)\n",
        "    return image, bbox, class_id"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ifdvKsYlR00"
      },
      "source": [
        "# Label Encoding:\n",
        "class LabelEncoder:\n",
        "  # turns raw labels from data to targets suitable for training\n",
        "  def __init__(self):\n",
        "    self._anchor_box = AnchorBox()\n",
        "    self._box_variance = tf.convert_to_tensor(\n",
        "        [0.1,0.1,0.2,0.2], dtype = tf.float32\n",
        "    )\n",
        "\n",
        "  def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou=0.5,ignore_iou=0.4):\n",
        "    # Uses Intercetion Over Union (IOU) to match Ground Truth (gt) boxes with Anchor boxes\n",
        "    # Steps:\n",
        "        # 1 -> Find pairwise IOU for each 'M' Anchor boxes and 'N' Ground Truth boxes and\n",
        "              # store in an [M,N] matrix.\n",
        "        # 2 -> If IOU > match_iou then the ground truth box with the maximum IOU is \n",
        "              # assigned to anchor box of each rox.\n",
        "        # 3 -> If max IOU in each row < ingore_iou then anchor box is set to background.\n",
        "        # 4 -> Any boxes not assigned a box is ingnored in training. \n",
        "    # Returns:\n",
        "      # matched_gt_idx:  Index of matched object.\n",
        "      # positive_mask:   Mask for anchor boxes set assigned to ground truth boxes.\n",
        "      # ignore_mask:     A mask for anchor boxes that need to be ignored.\n",
        "\n",
        "      # Step 1:\n",
        "      iou_matrix = intersection_over_union(anchor_boxes,gt_boxes)\n",
        "      # Step 2:\n",
        "      max_iou = tf.reduce_max(iou_matrix,axis=1)\n",
        "      matched_gt_idx = tf.argmax(iou_matrix,axis=1)\n",
        "      positive_mask = tf.greater_equal(max_iou,match_iou)\n",
        "      # Step 3:\n",
        "      negative_mask = tf.less(max_iou, ignore_iou)\n",
        "      # Step 4:\n",
        "      ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
        "      \n",
        "      return (\n",
        "            matched_gt_idx,\n",
        "            tf.cast(positive_mask, dtype=tf.float32),\n",
        "            tf.cast(ignore_mask, dtype=tf.float32),\n",
        "        )\n",
        "  \n",
        "  def _compute_box_target(self,anchor_boxes,matched_gt_boxes):\n",
        "    # Convert Ground Truth Boxes to labels for training:\n",
        "    box_target = tf.concat(\n",
        "    [\n",
        "        (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
        "            tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),],\n",
        "            axis=-1,)\n",
        "    box_target = box_target / self._box_variance\n",
        "    return box_target\n",
        "\n",
        "  def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
        "    # Creates bounding box and classification lables:\n",
        "    anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "    cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
        "    matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
        "            anchor_boxes, gt_boxes\n",
        "        )\n",
        "    matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
        "    box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
        "    matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
        "    cls_target = tf.where(\n",
        "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
        "        )\n",
        "    cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
        "    cls_target = tf.expand_dims(cls_target, axis=-1)\n",
        "    label = tf.concat([box_target, cls_target], axis=-1)\n",
        "    return label    \n",
        "\n",
        "  def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
        "    # Creates bounding boxes and classification labels for a whole batch.\n",
        "    images_shape = tf.shape(batch_images)\n",
        "    batch_size = images_shape[0]\n",
        "    labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
        "        labels = labels.write(i, label)\n",
        "    batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
        "    return batch_images, labels.stack()   "
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HWNUz5EmtbN"
      },
      "source": [
        "# Get base_model -> Resnet with pretrained weights:\n",
        "def get_base():\n",
        "  # returns feature maps from 3rd-5th layer\n",
        "  base_model = keras.applications.ResNet50(include_top=False,input_shape=[None,None,3])\n",
        "  c3_output, c4_output, c5_output = [\n",
        "    base_model.get_layer(layer_name).output\n",
        "      for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
        "    ]\n",
        "  return keras.Model(\n",
        "        inputs=[base_model.inputs], outputs=[c3_output, c4_output, c5_output]\n",
        "    )"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2CoU0XfouW_"
      },
      "source": [
        "# Feature Pyramid Layer:\n",
        "#   Constructs pyramid layers from base_model\n",
        "class FeaturePyramid(keras.layers.Layer):\n",
        "  def __init__(self,base=None,**kwargs):\n",
        "    super(FeaturePyramid,self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
        "    self.backbone = base if base else get_base()\n",
        "    self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "    self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "    self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
        "    self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "    self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "    self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
        "    self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "    self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
        "    self.upsample_2x = keras.layers.UpSampling2D(2)\n",
        "    \n",
        "  def call(self, images, training=False):\n",
        "    c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
        "    p3_output = self.conv_c3_1x1(c3_output)\n",
        "    p4_output = self.conv_c4_1x1(c4_output)\n",
        "    p5_output = self.conv_c5_1x1(c5_output)\n",
        "    p4_output = p4_output + self.upsample_2x(p5_output)\n",
        "    p3_output = p3_output + self.upsample_2x(p4_output)\n",
        "    p3_output = self.conv_c3_3x3(p3_output)\n",
        "    p4_output = self.conv_c4_3x3(p4_output)\n",
        "    p5_output = self.conv_c5_3x3(p5_output)\n",
        "    p6_output = self.conv_c6_3x3(c5_output)\n",
        "    p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
        "    return p3_output, p4_output, p5_output, p6_output, p7_output\n"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNl7qHrgtMgq"
      },
      "source": [
        "# Classification and Box Regression heads:\n",
        "def build_head(output_filters,bias_init):\n",
        "  # output_filters -> numbers of filters in final layers\n",
        "  # bias_init      -> initializer of final layer.\n",
        "  head = keras.Sequential([tf.keras.layers.InputLayer(input_shape=[None,None,256])])\n",
        "  kernel_init = tf.initializers.RandomNormal(0.0,0.01)\n",
        "\n",
        "  for _ in range(4):\n",
        "    head.add(keras.layers.Conv2D(256,3,padding=\"same\",kernel_initializer=kernel_init))\n",
        "    head.add(keras.layers.ReLU())\n",
        "    head.add(keras.layers.Conv2D(output_filters,3,1,padding=\"same\",\n",
        "                     kernel_initializer=kernel_init,bias_initializer=bias_init))\n",
        "    \n",
        "    return head"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUOfh4NOvKU-"
      },
      "source": [
        "# RetinaNet Model:\n",
        "class RetinaNet(keras.Model):\n",
        "  def __init__(self, num_classes,base=None,**kwargs):\n",
        "    super(RetinaNet,self).__init__(name=\"RetinaNet\",**kwargs)\n",
        "    self.fpn = FeaturePyramid(base)\n",
        "    self.num_classes = num_classes\n",
        "    prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
        "    self.cls_head = build_head(9 * num_classes, prior_probability)\n",
        "    self.box_head = build_head(9 * 4, \"zeros\")\n",
        "\n",
        "  def call(self,image,training=False):\n",
        "    features = self.fpn(image,training=training)\n",
        "    N = tf.shape(image)[0]\n",
        "    cls_outputs = []\n",
        "    box_outputs = []\n",
        "    \n",
        "    for feature in features:\n",
        "      box_outputs.append(tf.reshape(self.box_head(feature), [N,-1,4]))\n",
        "      cls_outputs.append(tf.reshape(self.cls_head(feature), [N,-1,self.num_classes]))\n",
        "\n",
        "    cls_outputs = tf.concat(cls_outputs,axis=1)\n",
        "    box_outputs = tf.concat(box_outputs,axis=1) \n",
        "\n",
        "    return tf.concat([box_outputs,cls_outputs], axis = -1)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOakJ4Qz05cR"
      },
      "source": [
        "class DecodePredictions(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_classes = 80,confidence_threshold=0.05,nms_iou_threshold=0.5,\n",
        "               max_detections_per_class = 100, max_detections=100,\n",
        "               box_variance =[0.1,0.1,0.2,0.2],**kwargs):\n",
        "    \n",
        "    super(DecodePredictions, self).__init__(**kwargs)\n",
        "    self.num_classes = num_classes\n",
        "    self.confidence_threshold  - confidence_threshold\n",
        "    self.nms_iou_threshold = nms_iou_threshold\n",
        "    self.max_detections_per_class = max_detections_per_class\n",
        "    self.max_detections = max_detections\n",
        "    self._anchor_box = AnchorBox\n",
        "    self.box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
        "\n",
        "\n",
        "  def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
        "    boxes = box_predictions * self._box_variance\n",
        "    boxes = tf.concat(\n",
        "            [\n",
        "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
        "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "    boxes_transformed = convert_to_coners(boxes)\n",
        "\n",
        "    return boxes_transformed\n",
        "\n",
        "  def call(self,images,predictions):\n",
        "    image_shape = tf.cast(tf.shape(images),dtype=tf.float32)\n",
        "    anchor_boxes = self.anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
        "    box_predictions = predictions[:,:,:4]\n",
        "    cls_predictions = tf.nn.sigmoid(predictions[:,:,:4])\n",
        "    boxes = self._decode_box_predictions(anchor_boxes[None,...], box_predictions)\n",
        " \n",
        "    return tf.image.combined_non_max_suppression(\n",
        "            tf.expand_dims(boxes, axis=2),\n",
        "            cls_predictions,\n",
        "            self.max_detections_per_class,\n",
        "            self.max_detections,\n",
        "            self.nms_iou_threshold,\n",
        "            self.confidence_threshold,\n",
        "            clip_boxes=False,\n",
        "        ) "
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn9AjFlI_FTU"
      },
      "source": [
        "# Model Losses:\n",
        "class RetinaNetBoxLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super(RetinaNetBoxLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
        "        )\n",
        "        self._delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        difference = y_true - y_pred\n",
        "        absolute_difference = tf.abs(difference)\n",
        "        squared_difference = difference ** 2\n",
        "        loss = tf.where(\n",
        "            tf.less(absolute_difference, self._delta),\n",
        "            0.5 * squared_difference,\n",
        "            absolute_difference - 0.5,\n",
        "        )\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
        "    \"\"\"Implements Focal loss\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, gamma):\n",
        "        super(RetinaNetClassificationLoss, self).__init__(\n",
        "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
        "        )\n",
        "        self._alpha = alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=y_pred\n",
        "        )\n",
        "        probs = tf.nn.sigmoid(y_pred)\n",
        "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
        "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
        "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "\n",
        "class RetinaNetLoss(tf.losses.Loss):\n",
        "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n",
        "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
        "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
        "        self._box_loss = RetinaNetBoxLoss(delta)\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        box_labels = y_true[:, :, :4]\n",
        "        box_predictions = y_pred[:, :, :4]\n",
        "        cls_labels = tf.one_hot(\n",
        "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
        "            depth=self._num_classes,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        cls_predictions = y_pred[:, :, 4:]\n",
        "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
        "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
        "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
        "        box_loss = self._box_loss(box_labels, box_predictions)\n",
        "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
        "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
        "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
        "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
        "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
        "        loss = clf_loss + box_loss\n",
        "        return loss\n"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T7RTdZgzlFJ"
      },
      "source": [
        "# Training parameters:\n",
        "\n",
        "model_dir = \"retinanet/\"\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "num_classes = 80\n",
        "batch_size = 2 \n",
        "\n",
        "learning_rates = [2.5e-06,0.000625,0.00125,0.00025,0.00025,2.5e-05]\n",
        "learning_rate_boundaries = [125,250,500,240000,360000]\n",
        "\n",
        "leraning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "                    boundaries=learning_rate_boundaries, values=learning_rates)\n",
        "\n"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWrsC2nK1xX3"
      },
      "source": [
        "# Compile and Initialize model:\n",
        "resnet50_base = get_base()\n",
        "loss_fn = RetinaNetLoss(num_classes)\n",
        "optimizer = tf.optimizers.SGD(learning_rate=leraning_rate_fn,momentum=0.9)\n",
        "model = RetinaNet(num_classes,resnet50_base)\n",
        "model.compile(loss=loss_fn,optimizer = optimizer)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh0T-L7704Cf"
      },
      "source": [
        "# Callbacks:\n",
        "callbacks_list = [\n",
        "                  tf.keras.callbacks.ModelCheckpoint(\n",
        "                      filepath=os.path.join(model_dir,\"weights\" + \"_epcoh_{epoch}\"),\n",
        "                      monitor=\"loss\",\n",
        "                      save_best_only=False,\n",
        "                      save_weights_only=True,\n",
        "                      verbose=1\n",
        "                      )\n",
        "                 ]"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjKBNUGx3JzJ"
      },
      "source": [
        "# Loading in COCO 2017:\n",
        "(training_set, validation_set), dataset_info = tfds.load(\n",
        "    \"coco/2017\",split=[\"train\", \"validation\"],with_info=True, data_dir = \"data\"\n",
        "     )"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe4qcnFi4RqG"
      },
      "source": [
        "# Data Pipeline:\n",
        "  # tf.data api allows us to load in images/batch data in parallel to  \n",
        "  # training steps. Speeding up overall computation.\n",
        "autotune = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Training_set PipeLine:\n",
        "training_set = training_set.map(preprocess_data,num_parallel_calls=autotune)\n",
        "training_set = training_set.shuffle(8 * batch_size)\n",
        "  # Pads images so that all training data is of equal size\n",
        "training_set = training_set.padded_batch(batch_size=batch_size,\n",
        "                           padding_values=(0.0,1e-8,-1),drop_remainder=True)\n",
        "  # Sets up labels for bounding boxes\n",
        "training_set = training_set.map(label_encoder.encode_batch,num_parallel_calls=autotune)\n",
        "training_set = training_set.apply(tf.data.experimental.ignore_errors())\n",
        "training_set = training_set.prefetch(autotune)\n",
        "\n",
        "validation_set = validation_set.map(preprocess_data,num_parallel_calls=autotune)\n",
        "validation_set = validation_set.padded_batch(\n",
        "    batch_size=1,padding_values=(0.0,1e-8,1),drop_remainder=True)\n",
        "      \n",
        "validation_set = validation_set.map(label_encoder.encode_batch,\n",
        "                                    num_parallel_calls=autotune)\n",
        "validation_set = validation_set.apply(tf.data.experimental.ignore_errors())\n",
        "validation_set = validation_set.prefetch(autotune)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cihfhMI-APM2",
        "outputId": "3706a5cc-d079-4d31-a659-d7f7ecf058ca"
      },
      "source": [
        "# Training:\n",
        "train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // batch_size\n",
        "val_steps_per_epoch = dataset_info.splits[\"validation\"].num_examples//batch_size\n",
        "\n",
        "train_steps = 4 * 100000\n",
        "epochs = train_steps // train_steps_per_epoch\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "model.fit(training_set.take(100),validation_data=validation_set.take(50),\n",
        "         epochs= epochs,callbacks=callbacks_list,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     25/Unknown - 157s 5s/step - loss: 4.6922"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}